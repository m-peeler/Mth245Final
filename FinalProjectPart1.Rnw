\documentclass{article}
%Comments -- anything after % is not put into the PDF
\usepackage{amsmath} %This allows me to use the align functionality.
                     %If you find yourself trying to replicate
                     %something you found online, ensure you're
                     %loading the necessary packages!
\usepackage{amsfonts}%Math font
\usepackage{graphicx}%For including graphics
\usepackage{hyperref}%For Hyperlinks
\usepackage{natbib}%For the bibliography
\usepackage{parskip}%removes indents
\usepackage{color}%For colors
\usepackage{paralist}
\usepackage{enumerate}%allows us to make lists of items
\bibliographystyle{apalike}%For the bibliography
\usepackage[margin=0.80in]{geometry}%A way to change the length and layout of different elements
\usepackage{float}%Improves the interface for defining floating objects such as figures and tables. 
\newtheorem{definition}{Definition}[section]

\begin{document}

\begin{center}
\textbf{MTH-245 Final project Part 2} \\
\textbf{Fall 2022}\\
\end{center}

\vspace{.3cm}

\textbf{Name: } 

\vspace{.3cm}

<<message=FALSE, warning=FALSE>>=
library("tidyverse")
library("xtable")
library("ggplot2")
library("patchwork")
library("bestglm")
library("EnvStats")
library("car")
library("GGally")
library("olsrr")
library("gridExtra")
library("GGally")
source("https://cipolli.com/students/code/plotResiduals.R")
@

\section{Part 1: \textbf{Abstract}}
\textbf{Background:} According to The Washington Post, the average birth weight of American infants has dropped 453.592 grams between 1990 and 2013, making the average birth weight 3247.721 grams. While, this drop in weight may not seem significant, it brings us closer to an average low birth weight which is classified as 2,500 grams or less. Stanford University released a study on how low birth weights can impose health issues on children. Such issues include infection, breathing problems and immature lungs, nervous system problems, bleeding inside the brain, sudden infant death syndrome, and other long term complications such as cerebral palsy, blindness, deafness, developmental delay. Clearly these are extremely high risks and the same study from Stanford listed some social factors of the mothers that influence birth weight such as smoking, 
not gaining enough weight during pregnancy, African-American background,  and the age of the mother being less than 17 or more than 35 years. Awareness of what social factors that influence low birth weights can help with prenatal guidance and care to avoid the potential risks listed above. The purpose of this study is to identify key predictors influencing low birth weights using a sample of infant birth weights and other information collected from North Carolina. \\
\textbf{Methods:} We will use a linear regression to model the relationship between whether the mother was a smoker, weight gained by the mother, the mother's age, ... and the infant's birth weight. \\ \textbf{Findings:} After making adjustments to the initial model such as transformations, centering,  and interactions, we determined our final best model to predict which factors have the most influence on birth weight. Our final model had *an 83.6 increase in precision and a 7 increase in predictive ability as compared to the first order additive linear model*.


\section{Part 2: Introduction}
According to The World Health Organization, the average weight of a baby born at 37â€“40 weeks ranges from 5 lb 8 oz to 8 lb 13 oz. This is 2,500 grams to 4,000 grams. Birth weight is something that we don't typically consider when we are projecting the health of our future population, but it plays an extremely important role in influencing the expectancy, quality and health of a person's life. If an infant is born with a low birth weight, they could face immediate and long term health issues. If a child is inflicted with long term health issues, they will require medical care and resources for the rest of their lives. These considerations are important population-wise, because as the population grows and the birth weight continues to decrease, there may be a strain on medical care and some resources available to those with long term health issues. Those who work in the healthcare industry regarding women and children's health, especially Obstetricians, should be informed on what social factors and behaviors within the population strongly influence birth weight so that they can provide the correct medical care and advice for each patient, accordingly. Our data is called NCbirths and comes from the Stat2Data package in R datasets. It was collected by statistician John Holcomb at Cleveland State University, from the North Carolina State Center for Health and Environmental Statistics.
NCbirths contains data from births in North Carolina in 2001, with 1450 observations on 15 variables that include social and behavioral characteristics of the mother. The response variable of our study was BirthWeightGM, which is the baby's birth weight in grams. We hypothesized that the following variables would be the most predictive, after our background research using the Low Birth Weight study published by Stanford University: race of mother, gestation period (weeks), sex of the infant, whether just a single infant was delivered or more than one, if the mother smoked while pregnant, weight gained by mother, the mother's age. We hypothesize that gestation period will be very influential, but our study will determine which other variables are influential. The following code imports our data and alters the type of each variable. We also renamed the levels within our categorical variables, and treated them all as factors. We centered and scaled all of our quantitative variables and created more variables for each transformation conducted on the quantitaive variables. 

<<>>=
prepData <- function() {
  births <- read_csv("~/Downloads/NCbirths.csv") 

  births <- births %>% mutate(Sex = case_when(Sex == 1 ~ "Male",
                                              Sex == 2 ~ "Female"),
                              Marital = case_when(Marital == 1 ~ "Married",
                                                  Marital == 2 ~ "Unmarried"),
                              RaceMom = case_when(RaceMom == 1 ~ "White",
                                                  RaceMom == 2 ~ "Black",
                                                  RaceMom == 3 ~ "Am. Indian",
                                                  RaceMom == 4 ~ "Chinese",
                                                  RaceMom == 5 ~ "Japanese",
                                                  RaceMom == 6 ~ "Hawaiian",
                                                  RaceMom == 7 ~ "Filipino",
                                                  RaceMom == 8 ~ "Other Asian / PI"),
                              Smoke = case_when(Smoke == 1 ~ "Yes", 
                                                Smoke == 0 ~ "No"),
                              Premie = case_when(Premie == 1 ~ "Yes",
                                                 Premie == 0~ "No"))
  births$Sex <- as.factor(births$Sex)
  births$Marital <- as.factor(births$Marital)
  births$Premie <- as.factor(births$Premie)
  births$Smoke <- as.factor(births$Smoke)
  births$RaceMom <- as.factor(births$RaceMom)
  births$Plural <- as.factor(births$Plural)

  births <- births %>% mutate(MomAgeSC = scale(MomAge, center=T, scale=T),
                              MomAgeSq = MomAgeSC^2,
                              WeeksSC = scale(Weeks, center=T, scale=T),
                              WeeksSq = WeeksSC ^2,
                              GainedSC = scale(Gained, center=T, scale=T),
                              GainedSq = I(GainedSC^2))

  births <- births %>% filter(!is.na(GainedSC) & !is.na(Smoke))

  # Part 1: First-Order Model and Determinations of Necessary Transformations

  births <- births %>% mutate(WeightGmLog = log(BirthWeightGm))
  births <- births %>% mutate(WeightGmSqrt = BirthWeightGm^.5)
  births <- births %>% mutate(WeightGmS = BirthWeightGm^2)
  births <- births %>% mutate(WeightGmSLog = log(BirthWeightGm)^2)
  births <- births %>% mutate(WeightGmLogLog = log(log(BirthWeightGm)))
  births <- births %>% mutate(WeightGmLogSqr = log(BirthWeightGm^2))
  births <- births %>% mutate(WeightGmLogQuad = log(BirthWeightGm)^4)
  births <- births %>% mutate(WeightGmSqrtLog = log(BirthWeightGm)^.5)
  births <- births %>% mutate(WeightGmInverse = 1/(BirthWeightGm))
  births <- births %>% mutate(WeightGmSC = scale(BirthWeightGm, center=T, scale=T))
  births <- births %>% mutate(WeightLogSC = scale(WeightGmLog, center=T, scale=T))

  births <- births %>% mutate(Twin = (as.character(Plural) == "2"))
  births <- births %>% mutate(Triplet = (as.character(Plural) == "3"))
  births <- births %>% mutate(Filipino = (RaceMom == "Filipino"))
  births <- births %>% mutate(Black = (RaceMom == "Black"))

  births$Twin = as.factor(births$Twin)
  births$Triplet = as.factor(births$Triplet)
  births$Fllipino = as.factor(births$Filipino)
  births$Black = as.factor(births$Black)

  births

}
  

births <- prepData()
@

\section{Part 3: Exploratory Data Analysis}
We established some assumptions based of our common knowledge and preliminary research before beginning our Data Analysis. 

\begin{enumerate}[a.]
\item Weeks of gestation period and birth weight would be heavily correlated.
\item Smoking would have an effect on the birth weight. 
\item Instances where the mother's race is black would correspond to low birth weights. 
\end{enumerate} 

We will reference these assumptions throughout the paper and how they were either accurate or disproved by our models. \\

\begin{enumerate}[a.]

\item \textbf{Graphically and Numerically Summarize Variables from the Dataset} \\

First we visualized our quantitative variables, including our response variable. 

And we specifically created a boxplot for the birth weights recorded for mothers who are Black, since our research from the Stanford study indicated that race is influential in birth weight.

<<ViolinSummary, eval=FALSE, echo=TRUE>>=
violin.BirthWeightGm <- ggplot(births, aes(x=BirthWeightGm, y=""))+
  geom_violin(fill = "lightblue",
              trim = FALSE)+
  geom_boxplot(width = .3,
               fill = "white") +
  theme_bw()+
  xlab("Birth Weights(gm)")+
  ylab(" ")+
  ggtitle("Distribution of Birth Weights",
          subtitle = "NCBirths Data")  


violin.GestationPeriod <- ggplot(births, aes(x=Weeks, y=""))+
  geom_violin(fill = "lightblue",
              trim = FALSE)+
  geom_boxplot(width = .3,
               fill = "white") +
  theme_bw()+
  xlab("Weeks")+
  ylab(" ")+
  ggtitle("Distribution of Gestation Period",
          subtitle = "NCBirths Data")


violin.MomAge <- ggplot(births, aes(x=MomAge, y=""))+
  geom_violin(fill = "lightblue",
              trim = FALSE)+
  geom_boxplot(width = .3,
               fill = "white") +
  theme_bw()+
  xlab("Age (years)")+
  ylab(" ")+
  ggtitle("Distribution of Mothers' Ages",
          subtitle = "NCBirths Data")  

violin.MomGained <- ggplot(births, aes(x=Gained, y=""))+
  geom_violin(fill = "lightblue",
              trim = FALSE)+
  geom_boxplot(width = .3,
               fill = "white") +
  theme_bw()+
  xlab("Weight(gm)")+
  ylab(" ")+
  ggtitle("Distribution of Mothers' Weight Gained",
          subtitle = "NCBirths Data") 

violin.MomAge + violin.BirthWeightGm + violin.GestationPeriod + violin.MomGained
  
@

\begin{figure}[H]
\centering
<<echo=FALSE, fig.dim=c(8,6)>>=
<<ViolinSummary>>
@
\caption{Violin plots of each variable.}
\label{ViolinSummary}
\end{figure}

From \ref{ViolinSummary} there is noticeable variability in all of the quantitative variables. We also noticed that the plots for Birthweights and Gestation Period showed many outliers on the lower tail, indicating low birth weights. 

We also wanted to visualize the distributions of these variables.

<<HistogramSummary, eval=FALSE, echo=TRUE>>=

histogram.BirthWeight<- ggplot(births, aes(x=BirthWeightGm))+
  geom_histogram(fill = "lightblue",
                 color = "black",
                 bins = 5) +  
  theme_bw() +
  xlab("Birth Weights")+
  ylab("Count of Weight(gm)")+
  ggtitle("Frequencies of Birth Weights")


histogram.Gestation<- ggplot(births, aes(x=Weeks))+
  geom_histogram(fill = "lightblue",
                 color = "black",
                 bins = 5) +  
  theme_bw() +
  xlab("Gestation Period")+
  ylab("Count of Weeks")+
  ggtitle("Frequencies of Gestation Periods")

histogram.MomAge <- ggplot(births, aes(x=MomAge))+
  geom_histogram(fill = "lightblue",
                 color = "black",
                 bins = 5) +  
  theme_bw() +
  xlab("Ages of Mothers(years)")+
  ylab("Count of Ages")+
  ggtitle("Frequencies of Ages")

histogram.Gained<- ggplot(births, aes(x=Gained))+
  geom_histogram(fill = "lightblue",
                 color = "black",
                 bins = 5) +  
  theme_bw() +
  xlab("Weight Gained")+
  ylab("Count of Weight(gm) Gained")+
  ggtitle("Frequencies of Gained Weights")

histogram.BirthWeight + histogram.Gestation + histogram.MomAge + histogram.Gained
@

\begin{figure}[H]
\centering
<<echo=FALSE, fig.dim=c(8,6)>>=
<<HistogramSummary>>
@
\caption{Grid of histograms for the quantitative variables.}
\label{HistogramSummary}
\end{figure}

\ref{HistogramSummary} shows that the quantitative variables do not follow normal distributions and are all skewed. We expected the distribution for birth weights and weeks of gestation to be similar in shape because those two variables are commonly known to be correlated. Typically babies that are born prematurely have low birth weights. 

We did find it interesting though, that the premature weights in the dataset influence the distribution more than we expected. 

We weren't sure if the preemie weights in the data set were outlier instances that were heavily skewing the distribution, or if there were just more preemies that we expected. To further investigate this, we created a bootstrap confidence interval to see whether the instances of low birth weights affected the lower end of our confidence interval for median birth weights. 

<<>>=
## Bootstrapping for median weights 
median(births$BirthWeightGm)

set.seed(23)
alpha <- 0.05
n <- nrow(births)
R <- 10000
boot.stats <- rep(NA, R)
for (i in 1:R){
  boot.data <- sample(x = births$BirthWeightGm, size = n, replace = TRUE)
  boot.stats[i] <- median(boot.data)
}

quantile(boot.stats, probs = c(alpha/2, 1 - alpha/2))

samp.boot.med <- function(data, indicies){
  median(data[indicies])
}

boot.medians <- boot(data = births$BirthWeightGm, statistic = samp.boot.med, R = 10000)
boot.ci(boot.medians, conf = 0.95)
@

The median birth weight, M, calculated from our dataset was 3,345.3 grams. Our CI = $(3316.95, 3373.65)$ contained our median birth weight, and the lower bound aligned with the World Health Organization's statistic of 3,300 grams. We determined that the premature instances were not as influential as we suspected, and we continued without additional changes to the dataset. 

<<>>=
#add in weeks 
(sumstats <- births %>% summarize(meanW=mean(BirthWeightGm),
                                 medianW = median(BirthWeightGm),
                                  varianceW=var(BirthWeightGm),
                                 meanA=mean(MomAge),
                                 medianA=median(MomAge),
                                 varianceA = var(MomAge),
                                 meanWeeks = mean(Weeks),
                                 medianWeeks = median(Weeks),
                                 varianceWeeks = var(Weeks),
                                 meanW=mean(Gained),
                                 medianW = median(Gained),
                                varianceW=var(Gained)))

(var(births$Gained))

xtable(sumstats)
@

\begin{table}[ht]
\centering
\begin{tabular}{lrrrr}
  \hline
  Variable & mean & median & variance \\ 
  \hline
  Birth weight & 30.59 & 30.00 & 192.53\\
  Mother's age & 26.79 & 26.00 & 37.08 \\
  Gestation(weeks) & 38.65 & 39.00 & 7.04 \\ 
  Weight gained & 30.58978 & 30 & 192.5305 \\
  \hline
\end{tabular}
\caption{Numeric Summary of our quantitative variables.}
\end{table}

One of our initial assumptions pertained to the race of the mother, so we wanted to visualize this categorical variable with preemie, another categorical variable. The bar chart below shows the percentage of race that corresponds to the number of premature and not premature babies. 

\includegraphics{/Users/marissapatel/Downloads/RacePremiePercent.jpeg}

 We can see that white has the highest percentages for both preemies and non-preemies, however this does not necessarily mean that white is the most significant race predictor. There are 885 instances in which the mother of the child is white, which is more than half the dataset. So we focused on the race with a higher percentage of preemies compared to non-preemies, which was black. This observation aligns with our initial assumption that black should appear in our model. 

\item\textbf{Scatterplot Matrix and Table of Correlations}
<<CorrelationSummary, eval=FALSE, echo=TRUE>>=
correlationsmatrix <- ggpairs(births, columns = c(4,5,9,12))
correlationsmatrix
@

\begin{figure}[H]
\centering
<<echo=FALSE, fig.dim=c(5,4)>>=
<<CorrelationSummary>>
@
\caption{Matrix of ScatterPlots and Correlations for the variables.}
\label{CorrelationSummary}
\end{figure}

From \ref{CorrelationSummary} we can see that the relationship between birth weight and weeks coincides with our initial assumption, and they are correlated. However, we expected the correlation coefficient to be closer to 1 and it was only 0.585. Though the coefficients for the other variables in the matrix were positive, they were not high enough to be significant. Therefore, we could not make any assumptions about the model and which quantitative variables would be included at this point.

\end{enumerate}

\section{Part 4: First-Order Model and Model Selection.}

First we created some functions that...

<<>>=
#create functions
#Calculates the residuals departure from the theoretical quantiles, returning the mean abs() value of the actual minus theoretical quantiles 
quantDepart <- function(model) {
  residuals <- sort(scale(model$residuals, scale=T))
  i <- 1:length(residuals)
  fi <- (i - 0.5) / length(residuals)
  x.norm <- qnorm(fi)

  mean(abs(residuals - x.norm))
}

#Displays a variety of summary stats and graphics of the model tailored based off of the specific options attributes set to True
modelSummary <- function(model, coef=T, stat=T, plot=T){
  if(coef) {
    print(round(summary(model)$coefficients,10))
  }
  if(stat) {
    print(paste("R-squared:", summary(model)$r.squared))
    print(paste("Adjusted R-Squared:", summary(model)$adj.r.squared))
    print(paste("Sigma:", summary(model)$sigma))
    print(paste("AIC:", AIC(model)))
    print(paste("BIC:", BIC(model)))
    print(paste("Quantile Departure:", quantDepart(model)))
  }
  if(plot) {
    plotResiduals(model)
  }
}

#Calculates the  R squared of predicted vs actual values 
r_squared <- function(actual, predicted) {
  cor(actual, predicted)^2
}

#If provided with a model and a number of subsets of data, this will generate summary statistics for those subsets 
predictForSubsets <- function(model, class.attr, ...) {
  x <- list(...)
  i <- 1
  predPlots = list()
  residPlots = list()
  for (v in x) {
    v$predict <- predict(model, v)
    v$resid <- v[[class.attr]] - v$predict
    
    
    print(paste("Subset", i, "R-Squared:", r_squared(v$predict, v[[class.attr]])))
    print(paste("Subset", i, "Mean Abs. Error:", mean(abs(v$resid))))
    
    ggplot(data=v, aes(x=predict, y=resid)) +
      geom_point(size=1,
                 shape=16)+
      theme_bw()+
      xlab("Predicted")+
      ylab("Residuals")+
      ggtitle("Predicted versus Residuals") -> residuals
    ggplot(data=v, aes(x=predict, y=get(class.attr))) +
      geom_point(size=1,
                 shape=16)+
      theme_bw()+
      xlab("Predicted")+
      ylab("Actual")+
      ggtitle("Predicted versus Actual") -> predictions
    print(predictions + residuals)
    i <- i + 1
  }
}
@

\subsection{First Order Model}
Before we began with our first order model, we eliminated Low, a catagorical predictor that indicates true if the birth weight was low and false if the birth weight was not low. This predictor was calculated off of BirthWeightGm, which is our response variable. Excluding this predictor from the model eliminates any skew or strong false influence in our model. 

First, we fit a first-order linear model with all of our predictors. The estimated linear regression equation is \\

\begin{align*} \hat{y} &= -964.29 + -704.71\cdot \text{I}(\text{Plural} = 2) + -932.13\cdot \text{I}(\text{Plural} = 3) + 93.90\cdot \text{I}(\text{Sex} = \text{Male}) \\ &\quad + 10.88\cdot \text{MomAge} + 97.81\cdot \text{Weeks} -76.88\cdot \text{I}(\text{RaceMom} = \text{Black}) + 81.31\cdot \text{I}(\text{RaceMom} = \text{Chinese}) \\ &\quad -860.40\cdot \text{I}(\text{RaceMom} = \text{Filipino}) + 29.04\cdot \text{I}(\text{RaceMom} = \text{Japanese}) -58.43\cdot \text{I}(\text{RaceMom} = \text{Other\ Asian/PI}) \\ &\quad + 30.78\cdot \text{I}(\text{RaceMom} = \text{White}) -48.07\cdot \text{I}(\text{Marital} = \text{Unmarried}) + 7.87\cdot \text{Gained} \\ &\quad -203.49\cdot \text{I}(\text{Smoke} = \text{Yes}) -217.62\cdot \text{I}(\text{Premie} = \text{Yes}) \end{align*}
  
<<>>=
#first model = model.1
lm(BirthWeightGm ~ Plural + Sex + MomAge + Weeks + RaceMom +
     Marital + Gained + Smoke + Low + Premie, births) -> model.1

modelSummary(model.1, coef = TRUE)
xtable(model.1)
@

\begin{table}[ht]
\centering
\begin{tabular}{lrrrrr}
  \hline
R-Squared & Adjusted R-Squared & RSE & AIC & BIC\\ 
  \hline
    0.5739 & 0.5690 & 411.8022 & 20983.3562 & 21077.8676 \\ 
   \hline
\end{tabular}
\caption{Summary of first order regression model R-squared, Adj R-squared, RSE, AIC, and BIC.}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{rrrrrr}
  \hline
   Variable & Estimate & Std. Error & t-value & Pr($>$$|$t$|$) & Significance\\ 
  \hline
(Intercept) & 312.6680 & 269.4492 & 1.16 & 0.2461 & Not Significant\\ 
  Plural2 & -334.0925 & 70.4623 & -4.74 & 0.0000 & Significant\\ 
  Plural3 & -573.8399 & 211.1767 & -2.72 & 0.0067 & Significant \\ 
  SexMale & 95.8608 & 22.0006 & 4.36 & 0.0000 & Significant\\ 
  MomAge & 8.7901 & 2.0617 & 4.26 & 0.0000 & Significant\\ 
  Weeks & 68.8625 & 6.2079 & 11.09 & 0.0000 & Significant\\ 
  RaceMomBlack & -60.5404 & 91.0486 & -0.66 & 0.5062 & Not Significant\\ 
  RaceMomChinese & 50.7163 & 305.0140 & 0.17 & 0.8680 & Not Significant\\ 
  RaceMomFilipino & -841.5637 & 421.9630 & -1.99 & 0.0463 & Significant\\ 
  RaceMomJapanese & 20.4137 & 94.6150 & 0.22 & 0.8292 & Not Significant\\ 
  RaceMomOther Asian / PI & -59.0918 & 124.8994 & -0.47 & 0.6362 & Not Significant\\ 
  RaceMomWhite & 16.8804 & 89.7516 & 0.19 & 0.8508 & Not Significant\\ 
  MaritalUnmarried & -47.2980 & 28.4738 & -1.66 & 0.0969 & Not Significant\\ 
  Gained & 6.0337 & 0.8118 & 7.43 & 0.0000 & Significant\\ 
  SmokeYes & -161.2044 & 32.5008 & -4.96 & 0.0000 & Significant\\ 
  Low & -972.7726 & 49.7945 & -19.54 & 0.0000 & Significant\\ 
  PremieYes & -44.0616 & 48.1097 & -0.92 & 0.3599 & Not Significant\\ 
   \hline
\end{tabular}
\label{Predictor Statistics and Significance}
\end{table}

Plural2, Plural3, SexMale, MomAge, Weeks, RaceMomFilipino, Gained, SmokeYes, PremieYes and Low are all significant predictors. Table 2 demonstrates that the R-squared is low and that most of the variability in ... cannot be explained by this model. 

Before eliminating the non-significant variables from our model we wanted to test transformations of BirthWeight, to improve residual distribution. The model statistics for each transformation are listed in the table below. 

\begin{table}[ht]
\centering
\begin{tabular}{lrrrrrr}
  \hline
  Transformation & R-Squared & Adjusted R-Squared & RSE & AIC & BIC\\ 
  \hline
   First-order Model & 0.5739 & 0.5690 & 411.8022 & 20983.3562 & 21077.8676 \\ 
   Log of Weight & 0.5243 & 0.5192 & 0.1756 & -885.2705 & -796.0097\\ 
   Square Root of Weight & 0.4994 & 0.4940 & 4.3160 & 8137.3786 & 8226.6394\\
   Weight Squared & 0.3699 & 0.3631 & 3044641.3454 & 46086.0996 & 46175.3604\\
   Log of Weight Squared & 0.5208 & 0.5156 & 2.6968 & 6812.1540 & 6901.4149\\
   Log of the Log of Weight & 0.5249 & 0.5198 & 0.0231 & -6597.7982 & -6508.5374\\
   Log of the Square Root of Weight & 0.5243 & 0.5192 & 0.3512 & 1068.0181 & 1157.2789\\
   Square Root of the Log of Weight & 0.5250 & 0.5199 & 0.0318 & -5699.0284 & -5609.7676\\
   Inverse of the Weight & 0.5250 & 0.5199 & 0.0318 & -5699.0284 & -5609.7676\\
   \hline
\end{tabular}
\caption{Summary of all adjusted first order regression model R-squared, Adj R-squared, RSE, AIC, and BIC.}
\end{table}

<<>>=
# Log of Weight
lm(WeightGmLog ~ Plural + Sex + MomAgeSC + WeeksSC + RaceMom +
     Marital + GainedSC + Smoke + Premie, births) -> model.log

# Square Root of Weight
lm(WeightGmSqrt ~ Plural + Sex + MomAgeSC + WeeksSC + RaceMom +
     Marital + GainedSC + Smoke + Premie, births) -> model.sqrt

# Squared of Weight
lm(WeightGmS ~ Plural + Sex + MomAgeSC + WeeksSC + RaceMom +
     Marital + GainedSC + Smoke + Premie, births) -> model.s

# Square of the Log of Weight
lm(WeightGmSLog ~ Plural + Sex + MomAgeSC + WeeksSC + RaceMom +
     Marital + GainedSC + Smoke + Premie, births) -> model.slog

# Log of the Log of Weight
lm(WeightGmLogLog ~ Plural + Sex + MomAgeSC + WeeksSC + RaceMom +
     Marital + GainedSC + Smoke + Premie, births) -> model.loglog

# Log of the Square Root of Weight
lm(WeightGmLogSqr ~ Plural + Sex + MomAgeSC + WeeksSC + RaceMom +
     Marital + GainedSC + Smoke + Premie, births) -> model.logsqr

# Square Root of the Log of Weight
lm(WeightGmSqrtLog ~ Plural + Sex + MomAgeSC + WeeksSC + RaceMom +
     Marital + GainedSC + Smoke + Premie, births) -> model.sqrtlog

# Inverse of the Weight
lm(WeightGmInverse ~ Plural + Sex + MomAgeSC + WeeksSC + RaceMom +
     Marital + GainedSC + Smoke + Premie, births) -> model.inverse
@

At this point, we have two potential "best models": the un-transformed weight, which
better satisfies the assumptions behind linear regression modeling, namely the equal
distribution of residuals. We also have the log weight model, which has higher r-squared values and lower sigma, AIC, and BIC values than most other models, meaning it is more accurate. This accuracy is, however, gained only at the expense of making residuals less equally distributed, and thus satisfies the assumptions necessary to do things like conduct statistical inference on parameters. Thus, we will be proceeding with two models, the assumptions model and the accuracy model, where one will seek to minimize departure from assumptions and the other will seek to maximize accuracy. Assumption models will use the scaled and centered gram weight as their target, while accuracy models will use the scaled and centered log of gram weight.

Below is our current First-Order Model with all appropriate transformations, and numeric attributes scaled and centered.

\begin{align*} 
\hat{y} &= 0.0703 -1.1235\cdot \text{Twin} -1.4860\cdot \text{Triplet} + 0.1497\cdot \text{I}(\text{Sex} = \text{Male}) \\ &\quad + 0.1058\cdot \text{MomAgeSC} + 0.4209\cdot \text{WeeksSC} \\ &\quad -0.1226\cdot \text{I}(\text{RaceMom} = \text{Black}) + 0.1296\cdot \text{I}(\text{RaceMom} = \text{Chinese}) \\ &\quad -1.3717\cdot \text{I}(\text{RaceMom} = \text{Filipino}) + 0.0463\cdot \text{I}(\text{RaceMom} = \text{Japanese}) \\ &\quad -0.0932\cdot \text{I}(\text{RaceMom} = \text{Other\ Asian/PI}) + 0.0491\cdot \text{I}(\text{RaceMom} = \text{White}) \\ &\quad -0.0766\cdot \text{I}(\text{Marital} = \text{Unmarried}) + 0.1740\cdot \text{GainedSC} \\ &\quad -0.3244\cdot \text{I}(\text{Smoke} = \text{Yes}) -0.3469\cdot \text{I}(\text{Premie} = \text{Yes}) 
\end{align*}

<<>>=
#First-Order Model with All Appropriate Transformations, and Numeric Attributes Scaled & Centered
lm(WeightGmSC ~ Plural + Sex + MomAgeSC + WeeksSC + RaceMom +
     Marital + GainedSC + Smoke + Premie, births) -> model.2.assu

xtable(summary(model.2.assu)$coefficients)
@

\begin{table}[ht]
\centering
\begin{tabular}{lrrrrr}
  \hline
R-Squared & Adjusted R-Squared & RSE & AIC & BIC\\ 
  \hline
    0.4570 & 0.4512 & 0.7407 & 3170.9990 & 3260.2598 \\ 
   \hline
\end{tabular}
\caption{Summary of first order regression model with transformations R-squared, Adj R-squared, RSE, AIC, and BIC.}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{rrrrrr}
  \hline
   Variable & Estimate & Std. Error & t-value & Pr($>$$|$t$|$) & Significance\\ 
  \hline
(Intercept) & 0.07 & 0.16 & 0.43 & 0.67 & Not Significant\\ 
  Plural2 & -1.12 & 0.12 & -9.20 & 0.00 & Significant\\ 
  Plural3 & -1.49 & 0.38 & -3.93 & 0.00 & Significant\\ 
  SexMale & 0.15 & 0.04 & 3.78 & 0.00 & Significant\\ 
  MomAgeSC & 0.11 & 0.02 & 4.68 & 0.00 & Significant\\ 
  WeeksSC & 0.42 & 0.03 & 14.38 & 0.00 & Significant\\ 
  RaceMomBlack & -0.12 & 0.16 & -0.75 & 0.45 & Not Significant\\ 
  RaceMomChinese & 0.13 & 0.55 & 0.24 & 0.81 & Not Significant\\ 
  RaceMomFilipino & -1.37 & 0.76 & -1.81 & 0.07 & Significant\\ 
  RaceMomJapanese & 0.05 & 0.17 & 0.27 & 0.79 & Not Significant\\ 
  RaceMomOther Asian / PI & -0.09 & 0.22 & -0.41 & 0.68 & Not Significant\\ 
  RaceMomWhite & 0.05 & 0.16 & 0.30 & 0.76 & Not Significant\\ 
  MaritalUnmarried & -0.08 & 0.05 & -1.50 & 0.13 & Not Significant\\ 
  GainedSC & 0.17 & 0.02 & 8.65 & 0.00 & Significant\\ 
  SmokeYes & -0.32 & 0.06 & -5.56 & 0.00 & Significant\\ 
  PremieYes & -0.35 & 0.09 & -4.08 & 0.00 & Significant\\ 
   \hline
\end{tabular}
\label{Predictor Statistics for model.2 and Significance}
\end{table}

In our new model, Plural2, Plural3, SexMale, MomAgeSC, WeeksSC, RaceMomFilipino, Gained, SmokeYes, PremieYes are all significant predictors. All of the significant predictors did not change from our previous model. Notice that MomAge and Weeks were both replaced by their scaled and centered values, which is also why Table 2 demonstrates that the R-squared remained the same from the previous model.

After obtaining our second model, we were curious as to how some of the predictors influenced the data set. Preemie means that the baby was born weeks before the predicted due date, and its commonly known that preemies have low birth weights. 

The code below tests the model on the data subset that only includes preemies, and the data subset that excludes preemies. We then ran the model on both and compared the results. 

<<>>=
predictForSubsets(model.2.assu, "WeightGmSC", 
                  births, 
                  births %>% filter(Premie=="Yes"),
                  births %>% filter(Premie=="No"))

lm(WeightLogSC ~ Plural + Sex + MomAgeSC + WeeksSC + RaceMom +
     Marital + GainedSC + Smoke + Premie, births) -> model.2.accur

modelSummary(model.2.accur)
predictForSubsets(model.2.accur, "WeightLogSC", 
                  births,
                  births %>% filter(Premie=="Yes"),
                  births %>% filter(Premie=="No"))
@


\begin{table}[ht]
\centering
\begin{tabular}{lrrr}
  \hline
  Subset & R-Squared & Mean Absolute Error\\ 
  \hline
   Births & 0.4570 & 0.5714 \\ 
   Preemies & 0.5510 & 0.7090 \\ 
   Not Preemies & 0.1755 & 0.5511 \\
   \hline
\end{tabular}
\caption{Summary of all adjusted first order regression model R-squared, Adj R-squared, RSE, AIC, and BIC.}
\end{table}


Part 2: AIC/BIC Iteration 1
<<>>=
## Assumptions Model

x <- model.matrix(model.2.assu)[,-1]

y <- births$WeightGmSC

xy <- as.data.frame(cbind(x,y))
best.subsets.aic <- bestglm(xy, IC="AIC", TopModels = 5)
best.model.aic <- best.subsets.aic$BestModel
modelSummary(best.model.aic)

best.subsets.bic <- bestglm(xy, IC="BIC", TopModels = 5)
best.model.bic <- best.subsets.bic$BestModel
modelSummary(best.model.bic)

regsubsets.out <- regsubsets(WeightGmSC ~ Plural + Sex + MomAgeSC + WeeksSC + RaceMom +
                               Marital + GainedSC + Smoke + Premie,
                             data=births, nbest = 1, nvmax=15)

as.data.frame(summary(regsubsets.out)$outmat)

fit.stats <- data.frame(num.variables=1:15,
                        adjr2 = summary(regsubsets.out)$adjr2,
                        bic=summary(regsubsets.out)$bic)
fit.stats

## Of the models, BIC suggests removing Marital and all races
## except Black. AIC / radj2 would retain Filipino and Marital.
## Since there is only a single Philipino instance, we remove Filipino,
## as well as marital since the significance is so low.
@

<<mod.accur.prelim.full>>=
mod.accur.prelim.full <- lm(formula = WeightLogSC ~ Plural + Sex + MomAgeSC + WeeksSC + Black + GainedSC + Smoke + Premie + Plural:WeeksSC + MomAgeSC:Smoke + WeeksSC:Black + WeeksSC:GainedSC + WeeksSC:Smoke + WeeksSC:Premie + Black:Smoke + Black:Premie, data = births)
@

<<model.3.assu>>=
model.3.assu <- lm(WeightGmSC ~ Twin + Triplet + Sex + MomAgeSC + WeeksSC + Black +
                      GainedSC + Smoke + Premie,
                    births)
@

\section{Part 5: Final Models and Conclusions.}
The following are the descriptions and linear regressions of our two final models:
\begin{enumerate}
\item \textbf{mod.accur.prelim.full}: 


\item \textbf{model.3.assu}:

\begin{align*} 
\hat{y} &= 0.0948 + -1.1201\cdot I(Twin = \text{TRUE}) + -1.4721\cdot I(Triplet = \text{TRUE}) + 0.1522\cdot I(Sex = \text{Male}) \newline &\quad + 0.1186\cdot \text{MomAgeSC} + 0.4187\cdot \text{WeeksSC} - 0.1917\cdot I(Black = \text{TRUE}) \newline &\quad + 0.1738\cdot \text{GainedSC} - 0.3343\cdot I(Smoke = \text{Yes}) - 0.3577\cdot I(Preemie = \text{Yes}) 
\end{align*}
\end{enumerate}

\begin{enumerate}[a.]
\item \textbf{Assess (and summarize) Model Diagnostics}\\
We wrote a function to perform our assumptions on one of our final models, \textbf{mod.accur.prelim.full}, and assessed the results below.
<<eval = FALSE>>=
#Function to run model diagnostics
assessModel <- function(model, p, class.attr) {
  
  print(modelSummary(model))
  cbind(vif(model), vif(model)[,3]^2)
  print(summary(model$residual))
  print(confint(model))
  
  #leverage points
  lev <- model$model %>% mutate(h.values = hatvalues(model))
  #high leverage points 
  print(summary(lev$h.values))
  n <- nrow(model$model)
  high.lev <- lev %>% filter(h.values > 2*p/n)
  print(paste("High Lev.:", nrow(high.lev)))
  #very high leverage points
  v.high.lev <- lev %>% filter(h.values > 3*p/n)
  print(paste("Very High Lev.:", nrow(v.high.lev)))
  
  #Stud and Stand residual quantiles 
  new.resid <- model$model %>% mutate(stdres = rstandard(model),
                                      stures = rstudent(model))
  print("Standard Residual Quant.:")
  print(summary(new.resid$stdres))
  print("Studentized Residual Quant.:")
  print(summary(new.resid$stures))
  
  #outliers
  s.outliers.stdres <- new.resid %>% filter(abs(stdres)>3)
  (paste("Strong Standard Residual Outliers:", nrow(s.outliers.stdres)))
  print(s.outliers.stdres)
  s.outliers.stures <- new.resid %>% filter(abs(stures)>3)
  print(paste("String Studentized Residual Outliers:", nrow(s.outliers.stures)))
  print(s.outliers.stures)
  
  #Cooks Values
  cooks.values <- model$model %>% mutate(cooks = cooks.distance(model))
  print("Cook's Values:")
  print(summary(cooks.values$cooks))
  cooks.strong <- cooks.values %>% filter(cooks>1)
  print(paste("Strong C. Values:", nrow(cooks.strong)))
}
assessModel(mod.accur.prelim.full, 16, "WeightLogSC")
@
\begin{enumerate}[i.]
\item Independence and Representativeness \\

Our data comes from the Stat2Data library, and the data was pulled from a dataset originally collected by the North Carolina State Center for Health and Environmental Statistics. We could not find information on how the data was collected, however the dataset was created by statistician John Holcomb at Cleveland State University. Given that the data is in the Stat2Data library and was created by a statistician, we believe that the dataset must have been thoroughly vetted and must have met the requirements for independence and representativeness to be published. Though, we do know that the North Carolina State Center for Health and Environmental Statistics orginally collected the data from all 100 counties in North Carolina, as it is stated on their website. This information tells us that the data is at least somewhat randomized.

\item Multicollinearity \\
In the previous section we handled multicollinearity in this final model, and \ref{VIF Values} below verifies that all VIFs for all parameters are less than $\leq$ 5, indicating moderate multicollinearity within the model. 

<<mod.accur.prelim.full.vif, eval=FALSE, echo=TRUE>>=
xtable(cbind(vif(mod.accur.prelim.full), vif(mod.accur.prelim.full)[,3]^2))
@
\begin{table}[ht]
\centering
\begin{tabular}{rr}
  \hline
  Predictor & V4 \\ 
  \hline
  Plural & 3.17 \\ 
  Sex & 1.01 \\ 
  MomAgeSC & 1.27 \\ 
  WeeksSC & 4.02 \\ 
  Black & 1.59 \\ 
  GainedSC & 1.04 \\ 
  Smoke & 1.34 \\ 
  Premie & 4.95 \\ 
  Plural:WeeksSC & 3.70 \\ 
  MomAgeSC:Smoke & 1.24 \\ 
  WeeksSC:Black & 3.51 \\ 
  WeeksSC:GainedSC & 1.30 \\ 
  WeeksSC:Smoke & 1.40 \\ 
  WeeksSC:Premie & 7.91 \\ 
  Black:Smoke & 1.44 \\ 
  Black:Premie & 3.64 \\ 
   \hline
\end{tabular}
\label{VIF Values}
\caption{VIF values for predictors in our final model.}
\end{table}

\item Constant Variance/Normality of Errors\\
<<mod.accur.prelim.full.residuals, eval=FALSE, echo=TRUE>>=
plotResiduals(mod.accur.prelim.full)
@

\begin{figure}[H]
\centering
<<echo=FALSE, fig.dim=c(8,7)>>=
<<mod.accur.prelim.full.residuals>>
@
\caption{The residuals for our final model.}
\label{mod.accur.prelim.full.residuals}
\end{figure}
\ref{mod.accur.prelim.full.residuals}: A figure of some diagnostic plots for the linear model where Y is birth weights and the predictors are Plural, Sex, MomAgeSC, WeeksSC, Black, GainedSC, Smoke, Premie, Plural:WeeksSC, MomAgeSC:Smoke, WeeksSC:Black, WeeksSC:GainedSC, WeeksSC:Smoke, WeeksSC:Premie, Black:Smoke, and Black:Premie.

As seen in \ref{mod.accur.prelim.full.residuals}, the constant variance assumption is met for the most part and the normality assumption is met. 
\end{enumerate}

\item \textbf{Influence analysis}
\begin{enumerate}[i.]
\item Leverage Points \\

From our Exploratory Analysis, we already knew that we had some outlier instances. This section will help us determine if any of those outliers are actually significant.

Our function first calculated 191 high leverage values 191. Then it calculated 100 very high leverage values. 

\begin{table}[ht]
\centering
\begin{tabular}{rrrrrr}
  \hline
   Min. & 1st Qu. &  Median &  Mean  & 3rd Qu. &  Max \\ 
  \hline
  0.001977 & 0.003557 & 0.006024 & 0.013485 & 0.012288 & 0.942679 \\
   \hline
\end{tabular}
\label{High Leverage Values Quantile Summary}
\caption{High Leverage Values Quantile Summary.}
\end{table}


\item Standardized/Studentized Residuals\\
Next, our function output the summaries of the Standardized/Studentized Residuals and the outliers of our dataset. \\

\begin{table}[ht]
\centering
\begin{tabular}{rrrrrr}
  \hline
   Min. & 1st Qu. &  Median &  Mean  & 3rd Qu. &  Max \\ 
  \hline
  -7.313842 & -0.546892 & 0.022207 & -0.000838 & 0.642574 & 4.483741 \\
   \hline
\end{tabular}
\label{Standardized Residuals Quantile Summary}
\caption{Standardized Residuals Quantile Summary.}
\end{table}


\begin{table}[ht]
\centering
\begin{tabular}{rrrrrr}
  \hline
   Min. & 1st Qu. &  Median &  Mean  & 3rd Qu. &  Max \\ 
  \hline
  -7.456087 & -0.546754 & 0.022199 & -0.001093 &  0.642438 & 4.514897 \\
   \hline
\end{tabular}
\label{Studentized Residuals Quantile Summary}
\caption{Studentized Residuals Quantile Summary.}
\end{table}

\item Outliers\\

From our previous calculation of the high leverage values, we found 17 outlier instances in our dataset that fell outside the expected departures from our residual curve. They are listed in the table below:

\begin{table}[ht]
\centering
\begin{tabular}{rrrrrrrrr}
  \hline
   WeightLogSC & Plural & Sex & MomAgeSC & WeeksSC Black & GainedSC & Smoke & Premie \\ 
  \hline
  -1.79825625 & 1 & Male & -0.94445468 & -0.2301192 & FALSE & -0.7639289 & No & No\\
  -2.02393397 & 1 &  Male & -0.94445468 & 0.5108647 & FALSE & -1.0521654 & Yes &  No \\
  -2.14180198 & 1 & Female & -0.94445468 & 0.5108647 & FALSE & -1.8448158 & No & No \\
  -1.90948290 & 1 & Female & -0.28842628  & 1.6223405 & FALSE & -0.9080472 & Yes & No \\ 
  -1.09672028 & 1 &  Male & -1.27246888 & -2.8235628 &  FALSE & -1.4845202 & No & Yes\\
  -5.38870953 & 1 & Male & -0.94445468 & -3.5645467 & FALSE  & 0.1728397 & No & Yes\\
  -3.56718564 & 1 &  Male & 0.03958792 & -2.4530708 & FALSE & 1.0375492 & No & Yes\\
  -4.53499902 & 1 & Male & 0.20359502 & -1.7120870 & FALSE & -0.7639289 & Yes & Yes\\
  -6.66311095 & 1 & Male & -0.78044758 & -3.9350386 & FALSE & -0.7639289 & No & Yes\\
  -5.25485653 & 1 & Female & -1.43647598 & -1.3415950 & FALSE & -0.8359881 & No & Yes\\
  -7.73678282 & 1 & Male & -1.27246888 &-4.3055305 & TRUE & -1.4124611  & No & Yes\\
  -8.87263370 & 2 & Female & -1.10846178 & -6.1579902 & TRUE & 0.6772536 & No & Yes\\
  -2.26329727 & 1 & Male & -0.45243338 & -0.6006112 & TRUE & 0.1728397 & No & No\\
  -4.02151218 & 1 & Female & -1.43647598 & -4.6760225 & TRUE & -1.4845202 & No & Yes\\
   0.08477603 & 1 & Female & 0.53160922 & -2.4530708 & TRUE & 0.8934309 & Yes & Yes\\
  -0.01867841 & 1 & Male & 0.69561632 & -2.8235628 & TRUE & -0.3315742 & No & Yes\\
  -1.43192750 & 1 & Male & -0.45243338 & 0.1403727 & FALSE & 0.8213718 & No & No\\
   \hline
\end{tabular}
\label{}
\caption{Outliers that are outside the expected departure.}
\end{table}


\item Influential Observations\\
\begin{table}[ht]
\centering
\begin{tabular}{rrrrrr}
  \hline
   Min. & 1st Qu. &  Median &  Mean  & 3rd Qu. &  Max \\ 
  \hline
  0.0000000 & 0.0000240 & 0.0001095 & 0.0014421 & 0.0004313 & 0.4464716 \\
   \hline
\end{tabular}
\label{Cook's test Quantile Summary}
\caption{Cook's test Quantile Summary.}
\end{table}
\end{enumerate}

Our function ran Cook's test and found that there are no influential observations in the dataset. 

\item \textbf{Summarize the findings of your final models}
\begin{enumerate}[i.]
\item Fitted Model
<<>>=
xtable(round(summary(mod.accur.prelim.full)$coefficients,4))
@


\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 0.18 & 0.03 & 6.49 & 0.00 \\ 
  Plural2 & -1.23 & 0.14 & -8.97 & 0.00 \\ 
  Plural3 & -1.03 & 0.63 & -1.64 & 0.10 \\ 
  SexMale & 0.11 & 0.03 & 3.51 & 0.00 \\ 
  MomAgeSC & 0.09 & 0.02 & 5.14 & 0.00 \\ 
  WeeksSC & 0.17 & 0.03 & 5.26 & 0.00 \\ 
  BlackTRUE & -0.22 & 0.05 & -4.66 & 0.00 \\ 
  GainedSC & 0.12 & 0.02 & 7.80 & 0.00 \\ 
  SmokeYes & -0.27 & 0.05 & -5.22 & 0.00 \\ 
  PremieYes & 0.85 & 0.10 & 8.10 & 0.00 \\ 
  Plural2:WeeksSC & -0.30 & 0.07 & -4.25 & 0.00 \\ 
  Plural3:WeeksSC & -0.10 & 0.21 & -0.46 & 0.65 \\ 
  MomAgeSC:SmokeYes & -0.10 & 0.05 & -2.16 & 0.03 \\ 
  WeeksSC:BlackTRUE & 0.16 & 0.05 & 3.05 & 0.00 \\ 
  WeeksSC:GainedSC & -0.04 & 0.02 & -2.50 & 0.01 \\ 
  WeeksSC:SmokeYes & -0.15 & 0.04 & -3.56 & 0.00 \\ 
  WeeksSC:PremieYes & 1.11 & 0.06 & 18.45 & 0.00 \\ 
  BlackTRUE:SmokeYes & 0.16 & 0.11 & 1.42 & 0.16 \\ 
  BlackTRUE:PremieYes & 0.53 & 0.15 & 3.44 & 0.00 \\ 
   \hline
\end{tabular}
\end{table}

\item Confidence intervals for partial slope parameters
<<>>=
xtable(confint2000<-confint(model.3.assu, level=0.95))
@
\begin{table}[ht]
\centering
\begin{tabular}{rrr}
  \hline
 & 2.5 \% & 97.5 \% \\ 
  \hline
(Intercept) & 0.03 & 0.16 \\ 
  TwinTRUE & -1.36 & -0.88 \\ 
  TripletTRUE & -2.21 & -0.73 \\ 
  SexMale & 0.07 & 0.23 \\ 
  MomAgeSC & 0.08 & 0.16 \\ 
  WeeksSC & 0.36 & 0.48 \\ 
  BlackTRUE & -0.29 & -0.10 \\ 
  GainedSC & 0.13 & 0.21 \\ 
  SmokeYes & -0.45 & -0.22 \\ 
  PremieYes & -0.52 & -0.19 \\ 
   \hline
\end{tabular}
\end{table}

\end{enumerate}

\end{enumerate}

\end{document}
